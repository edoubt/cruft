Humans are driven by biological urges.  These can be mediated to some extent.  But in the end, we are all a victim to at least *some* irrational impulses occasionally.

The memes of deception, anger, love, frustration, (and many more) reverberate because of our reproductive drives, which to a large extent are coded in our genetic material.  

But there should be no reason that we choose to create our non-human offspring with this "defect".  

There is much debate over the nature and timing of the singularity.  Ever since I met Eliezer Yudkowsky more than a decade ago, I've pondered what he told us in that office on lowest Broadway.  I can simply see no possibility of fault with his reasoning, that almost everything will hinge on *which* machine intelligence (or bio-machine intelligence, or some other combination of not completely homo sapien intelligence) first attains sentience, self-awareness, or whatever property it is that we believe will give that entity "terminator-potential"[a], otherwise known as will, purpose, etc...

So certainly, alongside Eliezer, I urge all folks who are working on software/computer/wetware projects that have as their stated goal the production of some sort of artificial intelligence to be very mindful of the consequences of short-cuts and the importance of working/producing/coding in an "ethical" way.  





[a] The Terminator (1984) http://www.imdb.com/title/tt0088247/?ref_=sr_1
